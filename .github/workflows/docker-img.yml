name: User-data

on:
  push:
    branches:
      - main
      - observ

  workflow_dispatch:


permissions:
  contents: read
  #id-token: write


env:
    AWS_ACCESS_KEY_ID : ${{ secrets.AWS_ACCESS_KEY_ID}}
    AWS_SECRET_ACCESS_KEY : ${{ secrets.AWS_SECRET_ACCESS_KEY}}
    AWS_REGION: us-east-1
    ECR_REPO: public.ecr.aws/a8e5a3z9/backend


# env:
#   AWS_REGION: us-east-1
#   POSTGRES_USER: postgres
#   POSTGRES_PASSWORD: postgres
#   POSTGRES_DB: testdb

jobs:
  app:
    name: Build or Use Existing Image & Deploy
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          #role-to-assume: arn:aws:iam::196031534418:role/GithubActions_youjay
          aws-region: ${{ env.AWS_REGION }}
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}


      - name: Check EKS Cluster Status with AWS CLI
        run: |
            CLUSTER_NAME="custom-eks" #Replace with your cluster name
            CLUSTER_STATUS=$(aws eks describe-cluster --name $CLUSTER_NAME --query 'cluster.status' --output text)
            echo "EKS Cluster Status: $CLUSTER_STATUS"
            if [ "$CLUSTER_STATUS" != "ACTIVE" ]; then
              echo "EKS cluster is not active!"
              exit 1
            fi
          
      - name: Login to Amazon ECR
        run: |
          aws ecr-public get-login-password --region ${{ env.AWS_REGION }} | \
          docker login --username AWS --password-stdin ${{ env.ECR_REPO }} 

          
      # - name: Detect App Code Changes
      #   id: detect
      #   run: |
      #     CHANGED=false
      #     if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ github.event.inputs.triggered_by_infra }}" == "true" ]]; then
      #       CHANGED=false
      #     else
      #       git fetch origin ${{ github.base_ref || 'main' }}
      #       CHANGED=$(git diff --name-only origin/${{ github.base_ref || 'main' }}...HEAD | grep '^backend/' || true)
      #     fi
      #     echo "app_changed=$CHANGED" >> $GITHUB_OUTPUT

      - name: Build & Push New Image
        # if: steps.detect.outputs.app_changed
        run: |
          IMAGE_TAG=latest
          #IMAGE_TAG=build-${{ github.sha }}
          docker build -t ${{ env.ECR_REPO }}:$IMAGE_TAG .
          docker push ${{ env.ECR_REPO }}:$IMAGE_TAG
          echo "IMAGE_TAG=$IMAGE_TAG" >> $GITHUB_ENV

      - name: Run Trivy vulnerability scan
        uses: aquasecurity/trivy-action@master # Use the official Trivy action
        with:
            image-ref: '${{ env.ECR_REPO }}:latest' # Specify the image to scan
            format: 'table' # Output format (e.g., table, json, sarif)
            exit-code: '1' # Fail the workflow if vulnerabilities are found
            ignore-unfixed: true # Ignore unfixed vulnerabilities
            severity: 'CRITICAL,HIGH' # Scan for specified severity levels
            # You can also add other options like:
            # cache-dir: '/tmp/.trivycache' # Specify cache directory
            #sarif-file: 'trivy-results.sarif' # Output SARIF file for GitHub Security tab
  
      #   # Optional: Upload scan results to GitHub Security tab (if using SARIF format)
      # - name: Upload Trivy scan results to GitHub Security tab
      #   if: always() && steps.trivy-scan.outputs.sarif-file != '' # Only run if SARIF file was generated
      #   uses: github/codeql-action/upload-sarif@v3
      #   with:
      #     sarif_file: 'trivy-results.sarif' # Path to your SARIF output file

      - name: Update image in backend.yaml
        run: |
          sed -i "s|image: .*|image: ${{ env.ECR_REPO }}:latest|" k8s/backend.yaml
          echo "Updated image in k8s/backend.yaml to latest"

      - name: Update Kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name custom-eks
          kubectl get nodes
      
      - name: Deploy to EKS
        run: |
          kubectl apply -f k8s/pg16.yaml
          kubectl apply -f k8s/pvc-eks.yaml
          kubectl apply -f k8s/backend.yaml
          #kubectl rollout restart deployment backend -n myapp
          kubectl rollout status deployment backend -n myapp
          kubectl get all -n myapp
          #kubectl set image deployment/backend backend-container=${{ env.ECR_REPO }}:$IMAGE_TAG -n myapp
          #kubectl rollout status deployment backend -n myapp 


      
      # - name: Test the Deployment 
      #   run: |
      #     # Wait for a few seconds to ensure the service is up
      #     sleep 15
      #     # Get the ClusterIP of the backend service
      #     CLUSTER_IP=$(kubectl get svc backend-service -n myapp -o jsonpath='{.spec.clusterIP}')
      #     echo "Cluster IP: $CLUSTER_IP"
      #     # Test the backend service endpoint
      #     RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" http://$CLUSTER_IP:8008/health)
      #     if [ "$RESPONSE" -eq 200 ]; then
      #       echo "Backend service is healthy!"
      #     else
      #       echo "Backend service is not reachable. HTTP Status: $RESPONSE"
      #       exit 1
      #     fi

      - name: Cleanup ECR (Optional)    
        run: |
          # Keep only the latest 3 images, delete older ones
          aws ecr-public describe-images --repository-name backend --query 'imageDetails[?imageTags[0]!=`null`]|sort_by(@,&imagePushedAt)[].imageTags[0]' --output text | \
          awk 'NR>3' | \
          xargs -I {} aws ecr-public batch-delete-image --repository-name backend --image-ids imageTag={}
          echo "Old images cleaned up from ECR."
